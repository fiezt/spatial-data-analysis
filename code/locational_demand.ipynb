{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "import pickle\n",
    "import utils\n",
    "import moran_auto\n",
    "import gmm\n",
    "import figure_functions\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import mixture\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "curr_dir = os.getcwd()\n",
    "data_path = curr_dir + '/../data/'\n",
    "fig_path = curr_dir + '/../figs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "park_data, gps_loc, N = utils.load_daily_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keys = park_data.index.get_level_values(1).unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = park_data.loc[(park_data['Day'] == 0) & (park_data['Hour'] == 10)]\n",
    "\n",
    "# Each row is an element key, and each column is a date.\n",
    "data = data_df['Load'].values.reshape((-1, N)).T\n",
    "\n",
    "P = data.shape[1]\n",
    "\n",
    "train = np.hstack((data[:, 8, None], gps_loc))\n",
    "\n",
    "# Saving the scaling so it can be applied to the test set as well.\n",
    "unscaled_loads = train[:,0]\n",
    "\n",
    "scaler = MinMaxScaler().fit(train)\n",
    "train = scaler.transform(train)\n",
    "\n",
    "gmm = mixture.GaussianMixture(n_init=200, n_components=4, \n",
    "                              covariance_type='diag').fit(train)\n",
    "\n",
    "# Scaling the mean and covariances back to GPS coordinates.\n",
    "means = np.vstack(([(mean[1:] - scaler.min_[1:])/(scaler.scale_[1:]) for mean in gmm.means_]))\n",
    "covs = np.dstack(([np.diag((cov[1:])/(scaler.scale_[1:]**2)) for cov in gmm.covariances_])).T\n",
    "\n",
    "train_labels = gmm.predict(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = moran_auto.get_mixture_weights(train_labels, len(data))\n",
    "I = moran_auto.moran_mixture(unscaled_loads, train_labels, len(data))\n",
    "var = moran_auto.moran_variance(unscaled_loads, w, len(data))\n",
    "e = moran_auto.moran_expectation(N)\n",
    "z = moran_auto.z_score(I, e, var)\n",
    "p_one_sided, p_two_sided = moran_auto.p_value(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.119248435147\n",
      "5.37163254567e-05\n",
      "-0.00392156862745\n",
      "16.8055142518\n",
      "1.11187822394e-63 2.22375644789e-63\n"
     ]
    }
   ],
   "source": [
    "print I\n",
    "print var\n",
    "print e\n",
    "print z\n",
    "print p_one_sided, p_two_sided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = moran_auto.get_adjacent_weights(keys, len(data), data_path)\n",
    "I = moran_auto.moran_adjacent(unscaled_loads, keys, len(data), data_path)\n",
    "var = moran_auto.moran_variance(unscaled_loads, w, len(data))\n",
    "e = moran_auto.moran_expectation(N)\n",
    "z = moran_auto.z_score(I, e, var)\n",
    "p_one_sided, p_two_sided = moran_auto.p_value(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0894593982559\n",
      "0.000852057230605\n",
      "-0.00392156862745\n",
      "3.19907176873\n",
      "0.000689354212739 0.00137870842548\n"
     ]
    }
   ],
   "source": [
    "print I\n",
    "print var\n",
    "print e\n",
    "print z\n",
    "print p_one_sided, p_two_sided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = gmm.locational_demand_analysis(park_data, gps_loc, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "days = [result[0] for result in results]\n",
    "hours = [result[1] for result in results]\n",
    "errors = [result[2] for result in results]\n",
    "morans = [result[3] for result in results]\n",
    "means = [result[4] for result in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_times = []\n",
    "for time in times:\n",
    "    if time % 10 + 8 in [8,9]:\n",
    "        continue\n",
    "        \n",
    "    good_times.append(time)\n",
    "    data = np.vstack((means[time]))\n",
    "    kmeans = KMeans(n_clusters=4).fit(data)\n",
    "    labels = kmeans.labels_.tolist()\n",
    "    \n",
    "#     print 'Time index is', time\n",
    "\n",
    "#     print len(np.where(np.array(labels) == 0)[0])\n",
    "#     print len(np.where(np.array(labels) == 1)[0])\n",
    "#     print len(np.where(np.array(labels) == 2)[0])\n",
    "#     print len(np.where(np.array(labels) == 3)[0])\n",
    "\n",
    "    plt.figure(figsize=(18,16))\n",
    "\n",
    "    colors = {0: 'blue', 1: 'red', 2: 'green', 3: 'orange'}\n",
    "\n",
    "#     for i in range(len(labels)):\n",
    "#         plt.scatter(data[i,0], data[i,1], color=colors[labels[i]], s=100)\n",
    "#         plt.scatter(data[i,0], data[i,1], color=colors[labels[i]], s=100)\n",
    "#         plt.scatter(data[i,0], data[i,1], color=colors[labels[i]], s=100)\n",
    "#         plt.scatter(data[i,0], data[i,1], color=colors[labels[i]], s=100)\n",
    "#     plt.show()\n",
    "    \n",
    "    if len(good_times) == 12:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(good_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = figure_functions.centroid_plots(means, gps_loc, N, times=good_times, fig_path=fig_path, shape=(3,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([1,2,5]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 4\n",
    "\n",
    "n = len(data)\n",
    "\n",
    "# Seed numpy random number generator.\n",
    "np.random.seed()\n",
    "\n",
    "# Choosing the first cluster centroid uniformly at random.\n",
    "init = np.random.choice(range(n), 1, replace=False).tolist()\n",
    "means = data[init]\n",
    "\n",
    "# Choosing the rest of the cluster centroid initializations.\n",
    "for i in range(1, k):\n",
    "\n",
    "    # Finding minimum squared distance for each point to an existing center.\n",
    "    squared_dists = np.array([[np.linalg.norm(data[j]-means[l])**2 \n",
    "                             for l in xrange(i)] for j in xrange(n)])\n",
    "\n",
    "    dist_mins = squared_dists.min(axis=1)\n",
    "\n",
    "    # Sampling with probability proportional to the minimum squared distance.\n",
    "    prob_weights = dist_mins/dist_mins.sum()\n",
    "    sample = np.random.multinomial(1, prob_weights).tolist()\n",
    "    sample_choice = sample.index(1)\n",
    "\n",
    "    means = np.vstack((means, data[sample_choice]))\n",
    "    \n",
    "\n",
    "squared_dists = np.array([[np.linalg.norm(data[j]-means[l])**2 \n",
    "                             for l in xrange(k)] for j in xrange(n)])\n",
    "min_max = abs(squared_dists.min(axis=1) - squared_dists.max(axis=1))\n",
    "min_max = min_max.reshape((-1, 1))\n",
    "min_max = np.hstack((min_max, np.arange(len(min_max)).reshape((-1,1))))\n",
    "min_max = min_max[min_max[:,0].argsort()][::-1]\n",
    "preferences = np.argmin(squared_dists, axis=1)\n",
    "old_labels = np.nan * np.zeros(n)\n",
    "\n",
    "removed_labels = []\n",
    "\n",
    "count = 0\n",
    "\n",
    "for i in xrange(n):\n",
    "    \n",
    "    while True:\n",
    "        if np.isnan(old_labels[int(min_max[count,1])]):\n",
    "            \n",
    "            old_labels[int(min_max[count,1])] = preferences[int(min_max[count,1])]\n",
    "        \n",
    "            count += 1\n",
    "            break\n",
    "        else:\n",
    "            count += 1\n",
    "    \n",
    "    for j in xrange(k):\n",
    "                \n",
    "        if list(old_labels).count(j) == n/k and j not in removed_labels:\n",
    "            \n",
    "            removed_labels.append(j)\n",
    "            \n",
    "            if len(removed_labels) == k:\n",
    "                break\n",
    "            \n",
    "            squared_dists = np.array([[np.linalg.norm(data[m] - means[l])**2 \n",
    "                                       for l in xrange(k)] for m in xrange(n)])\n",
    "            \n",
    "            squared_dists[:, removed_labels] = np.nan\n",
    "            \n",
    "            min_max = abs(np.nanmin(squared_dists, axis=1) - np.nanmax(squared_dists, axis=1))\n",
    "            \n",
    "            min_max = min_max.reshape((-1, 1))\n",
    "            \n",
    "            min_max = np.hstack((min_max, np.arange(len(min_max)).reshape((-1,1))))\n",
    "            \n",
    "            min_max = min_max[min_max[:,0].argsort()][::-1]\n",
    "            \n",
    "            preferences = np.nanargmin(squared_dists, axis=1)\n",
    "                        \n",
    "            count = 0\n",
    "            \n",
    "            break\n",
    "            \n",
    "plt.figure(figsize=(18,16))\n",
    "\n",
    "colors = {0: 'blue', 1: 'red', 2: 'green', 3: 'orange'}\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    plt.scatter(data[i,0], data[i,1], color=colors[old_labels[i]], s=100)\n",
    "    plt.scatter(data[i,0], data[i,1], color=colors[old_labels[i]], s=100)\n",
    "    plt.scatter(data[i,0], data[i,1], color=colors[old_labels[i]], s=100)\n",
    "    plt.scatter(data[i,0], data[i,1], color=colors[old_labels[i]], s=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "old_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "not_converged = True\n",
    "\n",
    "while not_converged:\n",
    "    means = np.array([data[np.where(old_labels == i)[0]].mean(axis=0).tolist() \n",
    "                      for i in xrange(k)])  \n",
    "    \n",
    "    new_squared_dists = np.array([[np.linalg.norm(data[j]-means[l])**2 \n",
    "                             for l in xrange(k)] for j in xrange(n)])\n",
    "    \n",
    "    new_preferences = np.argmin(new_squared_dists, axis=1)\n",
    "    \n",
    "    \n",
    "    sorted_dists = np.fliplr(np.sort(new_squared_dists, axis=1))\n",
    "    \n",
    "    delta = abs(sorted_dists[:, 0, None] - sorted_dists[:, 1, None])\n",
    "\n",
    "    delta = np.hstack((delta, np.arange(n).reshape((-1,1))))\n",
    "            \n",
    "    delta = delta[delta[:, 0].argsort()][::-1]        \n",
    "    \n",
    "    changes = []\n",
    "    \n",
    "    for idx in delta[:,1]:\n",
    "        if new_preferences[int(idx)] != old_labels[int(idx)]:\n",
    "            changes.append([idx, old_labels[int(idx)], new_preferences[int(idx)]])\n",
    "            \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_squared_dists[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
