{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have run the GetDataSDOT and you have data files containing transactions (e.g., 02142017_03232017.csv), then you can run this notebook to compute the loads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import calendar\n",
    "import glob\n",
    "import datetime\n",
    "import load_sdot_utils\n",
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "curr_path = os.getcwd()\n",
    "data_path = curr_path + '/../data'\n",
    "raw_transaction_path = data_path + '/RawTransactionData'\n",
    "belltown_path = data_path + '/Belltown_Minute'\n",
    "commcore_path = data_path + '/CommercialCore_Minute'\n",
    "pikepine_path = data_path + '/PikePine_Minute'\n",
    "firsthill_path = data_path + '/FirstHill_Minute'\n",
    "dennytriangle_path = data_path + '/DennyTriangle_Minute'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.read_csv(os.path.join(data_path, 'block_info.csv'))['PaidParkingArea'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load_sdot_utils.get_data(3, 2016, file_path=raw_transaction_path, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01012017_01312017.csv\n",
      "02012017_02282017.csv\n",
      "03012016_03312016.csv\n",
      "03012017_03312017.csv\n",
      "04012016_04302016.csv\n",
      "04012017_04302017.csv\n",
      "05012016_05312016.csv\n",
      "05012017_05312017.csv\n",
      "06012016_06302016.csv\n",
      "06012017_06302017.csv\n",
      "07012016_07312016.csv\n",
      "07012017_07312017.csv\n"
     ]
    }
   ],
   "source": [
    "filenames = []\n",
    "for fi in glob.glob(raw_transaction_path + '/*.csv'):\n",
    "    print fi.split('/')[-1]\n",
    "    filenames.append(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/tfiez/GitHub/spatial-data-analysis/code/../data/RawTransactionData/01012017_01312017.csv',\n",
       " '/Users/tfiez/GitHub/spatial-data-analysis/code/../data/RawTransactionData/02012017_02282017.csv',\n",
       " '/Users/tfiez/GitHub/spatial-data-analysis/code/../data/RawTransactionData/03012016_03312016.csv',\n",
       " '/Users/tfiez/GitHub/spatial-data-analysis/code/../data/RawTransactionData/03012017_03312017.csv',\n",
       " '/Users/tfiez/GitHub/spatial-data-analysis/code/../data/RawTransactionData/04012016_04302016.csv',\n",
       " '/Users/tfiez/GitHub/spatial-data-analysis/code/../data/RawTransactionData/04012017_04302017.csv',\n",
       " '/Users/tfiez/GitHub/spatial-data-analysis/code/../data/RawTransactionData/05012016_05312016.csv',\n",
       " '/Users/tfiez/GitHub/spatial-data-analysis/code/../data/RawTransactionData/05012017_05312017.csv',\n",
       " '/Users/tfiez/GitHub/spatial-data-analysis/code/../data/RawTransactionData/06012016_06302016.csv',\n",
       " '/Users/tfiez/GitHub/spatial-data-analysis/code/../data/RawTransactionData/06012017_06302017.csv',\n",
       " '/Users/tfiez/GitHub/spatial-data-analysis/code/../data/RawTransactionData/07012016_07312016.csv',\n",
       " '/Users/tfiez/GitHub/spatial-data-analysis/code/../data/RawTransactionData/07012017_07312017.csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames = [fi for fi in filenames if '2016' in fi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/tfiez/GitHub/spatial-data-analysis/code/../data/RawTransactionData/03012016_03312016.csv',\n",
       " '/Users/tfiez/GitHub/spatial-data-analysis/code/../data/RawTransactionData/04012016_04302016.csv',\n",
       " '/Users/tfiez/GitHub/spatial-data-analysis/code/../data/RawTransactionData/05012016_05312016.csv',\n",
       " '/Users/tfiez/GitHub/spatial-data-analysis/code/../data/RawTransactionData/06012016_06302016.csv',\n",
       " '/Users/tfiez/GitHub/spatial-data-analysis/code/../data/RawTransactionData/07012016_07312016.csv']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "months = [3,4,5,6,7]\n",
    "years = [2016,2016,2016,2016,2016]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subareas = ['First Hill', 'Denny Triangle', 'Commercial Core', 'Belltown', 'Pike-Pine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subareas = ['Belltown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting loads for month 3 and year 2016 for subarea Belltown\n"
     ]
    }
   ],
   "source": [
    "load_sdot_utils.create_loads(subareas, months, years, filenames, data_path, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_hour = 8\n",
    "end_hour = 20\n",
    "minute_interval = 60\n",
    "file_paths = [belltown_path]\n",
    "\n",
    "months = [3,4,5,6,7]\n",
    "years = [2016]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_sdot_utils.aggregate_loads(start_hour, end_hour, minute_interval, months, years, file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for path in file_paths:\n",
    "\n",
    "    # Creating new directory to write new files to if it does not exist.\n",
    "    subarea_name = path.split('/')[-1].split('_')[0]\n",
    "    subdir = path + '/..'\n",
    "    new_dir = subdir + '/' + subarea_name\n",
    "\n",
    "    if minute_interval == 1:\n",
    "        dir_addon = '_Minute'\n",
    "    elif minute_interval == 60:\n",
    "        dir_addon = '_Hour'\n",
    "    else:\n",
    "        dir_addon = '_' + str(minute_interval) + 'Minute' \n",
    "\n",
    "    new_dir += dir_addon\n",
    "\n",
    "    if not os.path.exists(new_dir):\n",
    "        os.makedirs(new_dir)\n",
    "\n",
    "    park_data = defaultdict(list)\n",
    "\n",
    "    month_map = {'Jan':1, 'Feb':2, 'Mar':3, 'Apr':4, 'May':5, 'Jun':6, 'Jul':7, \n",
    "                 'Aug':8, 'Sep':9, 'Oct':10, 'Nov':11, 'Dec':12}\n",
    "\n",
    "    for fi in glob.glob(path + '/*.csv'):\n",
    "        split = fi.split('/')[-1].split('-')\n",
    "        key = split[0]\n",
    "        date = split[1]\n",
    "        date = date[:4] + '-' + date[4:]\n",
    "        year, month = int(date.split('-')[0]), date.split('-')[1]\n",
    "        \n",
    "        if key == '13297' and month == 'Mar':\n",
    "            print year, month, fi\n",
    "            print 'breaking'\n",
    "            break\n",
    "        \n",
    "        if year not in years and month not in months:\n",
    "            continue\n",
    "\n",
    "        data = pd.read_csv(fi)\n",
    "        cols = data.columns\n",
    "        interval_data = []\n",
    "\n",
    "        # Aggregate from 1 minute intervals to specified interval.\n",
    "        for i in xrange(0, 1440, minute_interval):\n",
    "            interval = data.loc[i:i+minute_interval-1]\n",
    "            interval_avg = interval.values.mean(axis=0)\n",
    "            interval_data.append(interval_avg)\n",
    "\n",
    "        interval_data = np.vstack((interval_data))\n",
    "        interval_data = interval_data[start_hour*(60/minute_interval):end_hour*(60/minute_interval)]\n",
    "        interval_data = interval_data.T\n",
    "        interval_data = interval_data.flatten()\n",
    "\n",
    "        times = [(hour, minute) for hour in xrange(start_hour, end_hour) for minute in xrange(0, 60, minute_interval)]\n",
    "\n",
    "        index = [datetime.datetime(year, month_map[month], int(day), hour, minute, 0) for day in cols.tolist() for hour, minute in times]\n",
    "\n",
    "        new_df = pd.DataFrame(interval_data, index=index, columns=['Load'])\n",
    "        new_df.index.name = 'Datetime'\n",
    "        park_data[int(key)].append(new_df.copy())\n",
    "    break\n",
    "    for key in park_data:\n",
    "        park_data[key] = pd.concat(park_data[key], axis=0)\n",
    "        park_data[key] = park_data[key].sort_index(axis=0)\n",
    "        park_data[key] = park_data[key].reset_index()\n",
    "        park_data[key].to_csv(os.path.join(new_dir, str(key)+'.csv'), sep=',', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check = pd.read_csv(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check.loc[check['1'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
